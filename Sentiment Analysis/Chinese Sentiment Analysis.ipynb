{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries Needed**  \n",
    "numpy\n",
    "jieba\n",
    "gensim\n",
    "tensorflow\n",
    "matplotlib\n",
    "sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pretrained Word Vectors**  \n",
    "使用了北京师范大学中文信息处理研究所与中国人民大学 DBIIR 实验室的研究者开源的Chinese-Word-Vectors：  \n",
    "https://github.com/Embedding/Chinese-Word-Vectors  \n",
    "word2vec的文章：  \n",
    "https://zhuanlan.zhihu.com/p/26306795  \n",
    "我们使用了\"chinese-word-vectors\"知乎Word+Ngram的词向量，可以从上面github链接下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用gensim加载预训练中文分词embedding\n",
    "cn_model = KeyedVectors.load_word2vec_format('../sgns.zhihu.bigram',binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Vectors Model**  \n",
    "在这个词向量模型里，每一个词是一个索引，对应的是一个长度为300的向量。LSTM并不能直接处理汉字文本，需要先进行分次并把词汇转换为词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每一个词都对应一个长度为300的向量\n",
    "embedding_dim = cn_model['孔子'].shape[0]\n",
    "# print('词向量的长度为{}'.format(embedding_dim))\n",
    "# cn_model['孔子']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算相似度\n",
    "# cn_model.similarity('孔子', '庄子')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出最相近的词，余弦相似度\n",
    "# cn_model.most_similar(positive=['中国'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 找出不同的词\n",
    "# test_words = '孔丘 圣人 孔子 孟子 孔子 荀子'\n",
    "# test_words_result = cn_model.doesnt_match(test_words.split())\n",
    "# print('在 '+test_words+' 中:\\n不是同一类别的词为: %s' %test_words_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cn_model.most_similar(positive=['女人','出轨'], negative=['男人'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 样本存放于两个文件夹中，分别为正面评价文件夹'pos'和负面评价文件夹'neg'\n",
    "# 每个文件夹中有txt文件，每个文件中是一例评价\n",
    "import os\n",
    "pos_txts = os.listdir('Questionnaire Review/pos')\n",
    "neg_txts = os.listdir('Questionnaire Review/neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print('样本总共: '+ str(len(pos_txts) + len(neg_txts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们将所有的评价内容放置到一个list里\n",
    "train_texts_orig = []\n",
    "for i in range(len(pos_txts)):\n",
    "    with open('Questionnaire Review/pos/'+pos_txts[i], 'r', encoding='gbk') as f:\n",
    "        text = f.read().strip()\n",
    "        train_texts_orig.append(text)\n",
    "        f.close()\n",
    "for i in range(len(neg_txts)):\n",
    "    with open('Questionnaire Review/neg/'+neg_txts[i], 'r', encoding='gbk') as f:\n",
    "        text = f.read().strip()\n",
    "        train_texts_orig.append(text)\n",
    "        f.close()\n",
    "# 添加完所有样本之后，train_texts_orig为一个含有文本的list，其中先是正面评价，后为负面评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不切合实际？异想天开\n"
     ]
    }
   ],
   "source": [
    "# Verify integrity of original data\n",
    "# print(len(train_texts_orig))\n",
    "print(train_texts_orig[922])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用tensorflow的keras接口来建模\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分词和tokenize**  \n",
    "首先我们去掉每个样本的标点符号，然后用结巴分词，结巴分词返回一个生成器，没法直接进行tokenize，所以我们将分词结果转换成一个list，并将它索引化，这样每一例评价的文本变成一段索引数字，对应着预训练词向量模型中的词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行分词和tokenize\n",
    "# train_tokens是一个长list，其中含有多个小list，对应每一条评价\n",
    "train_tokens = []\n",
    "for text in train_texts_orig:\n",
    "    # 去掉标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 结巴分词\n",
    "    cut = jieba.cut(text)\n",
    "    # 结巴分词的输出结果为一个生成器\n",
    "    # 把生成器转换为list\n",
    "    cut_list = [ i for i in cut ]\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**索引长度标准化**  \n",
    "因为每段评语的长度是不一样的，我们如果单纯取最长的一个评语，并把其他评填充成同样的长度，这样十分浪费计算资源，所以我们取一个折衷的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得所有tokens的长度\n",
    "num_tokens = [len(tokens) for tokens in train_tokens]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 平均tokens的长度\n",
    "# np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最长的评价tokens的长度\n",
    "# np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(np.log(num_tokens), bins = 100)\n",
    "# plt.xlim((0,5))\n",
    "# plt.ylabel('number of tokens')\n",
    "# plt.xlabel('length of tokens')\n",
    "# plt.title('Distribution of tokens length')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "max_tokens = np.mean(num_tokens) + 4 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "# max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大约95%的样本被涵盖\n",
    "# 我们对长度不足的进行padding，超长的进行修剪\n",
    "# np.sum( num_tokens < max_tokens ) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**反向tokenize**  \n",
    "我们定义一个function，用来把索引转换成可阅读的文本，这对于debug很重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来将tokens转换为文本\n",
    "def reverse_tokens(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "            text = text + cn_model.index2word[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse = reverse_tokens(train_tokens[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 经过tokenize再恢复成文本\n",
    "# 可见标点符号都没有了\n",
    "# reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 原始文本\n",
    "# train_texts_orig[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**准备Embedding Matrix**  \n",
    "现在我们来为模型准备embedding matrix（词向量矩阵），根据keras的要求，我们需要准备一个维度为$(numwords, embeddingdim)$的矩阵，num words代表我们使用的词汇的数量，emdedding dimension在我们现在使用的预训练词向量模型中是300，每一个词汇都用一个长度为300的向量表示。  \n",
    "注意我们只选择使用前100k个使用频率最高的词，在这个预训练词向量模型中，一共有260万词汇量，如果全部使用在分类问题上会很浪费计算资源，因为我们的训练样本很小，一共只有4k，如果我们有100k，200k甚至更多的训练样本时，在分类问题上可以考虑减少使用的词汇量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只使用前100000个词\n",
    "num_words = 100000\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# 维度为 100000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查index是否对应，\n",
    "# 输出300意义为长度为300的embedding向量一一对应\n",
    "# np.sum( cn_model[cn_model.index2word[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix的维度，\n",
    "# 这个维度为keras的要求，后续会在模型中用到\n",
    "# embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**padding（填充）和truncating（修剪）**  \n",
    "我们把文本转换为tokens（索引）之后，每一串索引的长度并不相等，所以为了方便模型的训练我们需要把索引的长度标准化，上面我们选择了236这个可以涵盖95%训练样本的长度，接下来我们进行padding和truncating，我们一般采用'pre'的方法，这会在文本索引的前面填充0，因为根据一些研究资料中的实践，如果在文本索引后面填充0的话，会对模型造成一些不良影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行padding和truncating， 输入的train_tokens是一个list\n",
    "# 返回的train_pad是一个numpy array\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超出五万个词向量的词用0代替\n",
    "train_pad[train_pad>=num_words] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 可见padding之后前面的tokens全变成0，文本在最后面\n",
    "# train_pad[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备target向量，前641样本为1，后348为0\n",
    "train_target = np.concatenate((np.ones(641),np.zeros(348)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行训练和测试样本的分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90%的样本用来训练，剩余10%用来测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad, train_target, test_size=0.1, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       去环游世界\n",
      "class:  1.0\n"
     ]
    }
   ],
   "source": [
    "# 查看训练样本，确认无误\n",
    "print(reverse_tokens(X_train[123]))\n",
    "print('class: ',y_train[123])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们用keras搭建LSTM模型，模型的第一层是Embedding层，只有当我们把tokens索引转换为词向量矩阵之后，才可以用神经网络对文本进行处理。\n",
    "keras提供了Embedding接口，避免了繁琐的稀疏矩阵操作。   \n",
    "在Embedding层我们输入的矩阵为：$$(batchsize, maxtokens)$$\n",
    "输出矩阵为： $$(batchsize, maxtokens, embeddingdim)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用LSTM对样本进行分类\n",
    "model = Sequential()\n",
    "\n",
    "# 模型第一层为Embedding\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Bidirectional(LSTM(units=32, return_sequences=True)))\n",
    "model.add(LSTM(units=16, return_sequences=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**构建模型**  \n",
    "我在这个教程中尝试了几种神经网络结构，因为训练样本比较少，所以我们可以尽情尝试，训练过程等待时间并不长：  \n",
    "**GRU：**如果使用GRU的话，测试样本可以达到87%的准确率，但我测试自己的文本内容时发现，GRU最后一层激活函数的输出都在0.5左右，说明模型的判断不是很明确，信心比较低，而且经过测试发现模型对于否定句的判断有时会失误，我们期望对于负面样本输出接近0，正面样本接近1而不是都徘徊于0.5之间。  \n",
    "**BiLSTM：**测试了LSTM和BiLSTM，发现BiLSTM的表现最好，LSTM的表现略好于GRU，这可能是因为BiLSTM对于比较长的句子结构有更好的记忆，有兴趣的朋友可以深入研究一下。  \n",
    "Embedding之后第，一层我们用BiLSTM返回sequences，然后第二层16个单元的LSTM不返回sequences，只返回最终结果，最后是一个全链接层，用sigmoid激活函数输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# 我们使用adam以0.001的learning rate进行优化\n",
    "optimizer = Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 300)           30000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 25, 64)            85248     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 16)                5184      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 30,090,449\n",
      "Trainable params: 90,449\n",
      "Non-trainable params: 30,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 模型的结构\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "path_checkpoint = 'sentiment_checkpoint.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试加载已训练模型\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义early stoping如果3个epoch内validation loss没有改善则停止训练\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动降低learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-5, patience=0,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义callback函数\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 801 samples, validate on 89 samples\n",
      "Epoch 1/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9137\n",
      "Epoch 00001: val_loss improved from 0.47487 to 0.15989, saving model to sentiment_checkpoint.keras\n",
      "801/801 [==============================] - 45s 56ms/sample - loss: 0.2289 - acc: 0.9126 - val_loss: 0.1599 - val_acc: 0.9326\n",
      "Epoch 2/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9125- ETA: 1s - loss: 0.\n",
      "Epoch 00002: val_loss improved from 0.15989 to 0.15954, saving model to sentiment_checkpoint.keras\n",
      "801/801 [==============================] - 50s 63ms/sample - loss: 0.2244 - acc: 0.9126 - val_loss: 0.1595 - val_acc: 0.9326\n",
      "Epoch 3/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9125\n",
      "Epoch 00003: val_loss did not improve from 0.15954\n",
      "801/801 [==============================] - 47s 58ms/sample - loss: 0.2207 - acc: 0.9126 - val_loss: 0.1597 - val_acc: 0.9326\n",
      "Epoch 4/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.2175 - acc: 0.9137\n",
      "Epoch 00004: val_loss improved from 0.15954 to 0.15951, saving model to sentiment_checkpoint.keras\n",
      "801/801 [==============================] - 46s 58ms/sample - loss: 0.2173 - acc: 0.9139 - val_loss: 0.1595 - val_acc: 0.9326\n",
      "Epoch 5/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.2142 - acc: 0.9150- E - ETA: 0s - loss: 0.2149 - acc: 0.9\n",
      "Epoch 00005: val_loss improved from 0.15951 to 0.15940, saving model to sentiment_checkpoint.keras\n",
      "801/801 [==============================] - 47s 58ms/sample - loss: 0.2140 - acc: 0.9151 - val_loss: 0.1594 - val_acc: 0.9326\n",
      "Epoch 6/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9187\n",
      "Epoch 00006: val_loss did not improve from 0.15940\n",
      "801/801 [==============================] - 49s 62ms/sample - loss: 0.2108 - acc: 0.9176 - val_loss: 0.1594 - val_acc: 0.9326\n",
      "Epoch 7/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9187\n",
      "Epoch 00007: val_loss improved from 0.15940 to 0.15923, saving model to sentiment_checkpoint.keras\n",
      "801/801 [==============================] - 51s 64ms/sample - loss: 0.2078 - acc: 0.9189 - val_loss: 0.1592 - val_acc: 0.9551\n",
      "Epoch 8/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9212\n",
      "Epoch 00008: val_loss improved from 0.15923 to 0.15904, saving model to sentiment_checkpoint.keras\n",
      "801/801 [==============================] - 51s 64ms/sample - loss: 0.2048 - acc: 0.9213 - val_loss: 0.1590 - val_acc: 0.9551\n",
      "Epoch 9/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9250\n",
      "Epoch 00009: val_loss improved from 0.15904 to 0.15875, saving model to sentiment_checkpoint.keras\n",
      "801/801 [==============================] - 51s 63ms/sample - loss: 0.2018 - acc: 0.9251 - val_loss: 0.1588 - val_acc: 0.9551\n",
      "Epoch 10/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9262- ETA: 1\n",
      "Epoch 00010: val_loss improved from 0.15875 to 0.15861, saving model to sentiment_checkpoint.keras\n",
      "801/801 [==============================] - 51s 64ms/sample - loss: 0.1988 - acc: 0.9263 - val_loss: 0.1586 - val_acc: 0.9551\n",
      "Epoch 11/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.9275\n",
      "Epoch 00011: val_loss improved from 0.15861 to 0.15842, saving model to sentiment_checkpoint.keras\n",
      "801/801 [==============================] - 51s 64ms/sample - loss: 0.1960 - acc: 0.9276 - val_loss: 0.1584 - val_acc: 0.9551\n",
      "Epoch 12/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.1933 - acc: 0.9300\n",
      "Epoch 00012: val_loss improved from 0.15842 to 0.15814, saving model to sentiment_checkpoint.keras\n",
      "801/801 [==============================] - 51s 64ms/sample - loss: 0.1932 - acc: 0.9301 - val_loss: 0.1581 - val_acc: 0.9551\n",
      "Epoch 13/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9312- ETA: 0s - loss: 0.1895 - ac\n",
      "Epoch 00013: val_loss improved from 0.15814 to 0.15777, saving model to sentiment_checkpoint.keras\n",
      "801/801 [==============================] - 52s 64ms/sample - loss: 0.1904 - acc: 0.9313 - val_loss: 0.1578 - val_acc: 0.9551\n",
      "Epoch 14/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9312\n",
      "Epoch 00014: val_loss improved from 0.15777 to 0.15763, saving model to sentiment_checkpoint.keras\n",
      "801/801 [==============================] - 51s 64ms/sample - loss: 0.1877 - acc: 0.9313 - val_loss: 0.1576 - val_acc: 0.9551\n",
      "Epoch 15/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9300\n",
      "Epoch 00015: val_loss improved from 0.15763 to 0.15741, saving model to sentiment_checkpoint.keras\n",
      "801/801 [==============================] - 51s 64ms/sample - loss: 0.1849 - acc: 0.9301 - val_loss: 0.1574 - val_acc: 0.9551\n",
      "Epoch 16/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.1819 - acc: 0.9325- ETA: 1s - loss: 0.184\n",
      "Epoch 00016: val_loss improved from 0.15741 to 0.15703, saving model to sentiment_checkpoint.keras\n",
      "801/801 [==============================] - 51s 64ms/sample - loss: 0.1823 - acc: 0.9326 - val_loss: 0.1570 - val_acc: 0.9551\n",
      "Epoch 17/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.1798 - acc: 0.9337\n",
      "Epoch 00017: val_loss improved from 0.15703 to 0.15656, saving model to sentiment_checkpoint.keras\n",
      "801/801 [==============================] - 51s 64ms/sample - loss: 0.1796 - acc: 0.9338 - val_loss: 0.1566 - val_acc: 0.9551\n",
      "Epoch 18/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9350- ETA: 0s - loss: 0.1739 -\n",
      "Epoch 00018: val_loss improved from 0.15656 to 0.15607, saving model to sentiment_checkpoint.keras\n",
      "801/801 [==============================] - 51s 64ms/sample - loss: 0.1770 - acc: 0.9351 - val_loss: 0.1561 - val_acc: 0.9551\n",
      "Epoch 19/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9362\n",
      "Epoch 00019: val_loss improved from 0.15607 to 0.15597, saving model to sentiment_checkpoint.keras\n",
      "801/801 [==============================] - 51s 64ms/sample - loss: 0.1745 - acc: 0.9363 - val_loss: 0.1560 - val_acc: 0.9551\n",
      "Epoch 20/20\n",
      "800/801 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9362\n",
      "Epoch 00020: val_loss improved from 0.15597 to 0.15559, saving model to sentiment_checkpoint.keras\n",
      "801/801 [==============================] - 51s 64ms/sample - loss: 0.1719 - acc: 0.9363 - val_loss: 0.1556 - val_acc: 0.9551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dbbc409208>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          validation_split=0.1, \n",
    "          epochs=20,\n",
    "          batch_size=1,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**结论**  \n",
    "我们首先对测试样本进行预测，得到了还算满意的准确度。  \n",
    "之后我们定义一个预测函数，来预测输入的文本的极性，可见模型对于否定句和一些简单的逻辑结构都可以进行准确的判断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 0s 747us/sample - loss: 0.2455 - acc: 0.9091\n",
      "Accuracy:0.90909094\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:'+str(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('是一例正面评价','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('是一例负面评价','output=%.2f'%coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_coef(text):\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我觉得不好\n",
      "是一例负面评价 output=0.12\n",
      "我觉得很好\n",
      "是一例正面评价 output=0.92\n",
      "我被孤立了\n",
      "是一例负面评价 output=0.01\n",
      "我的心情有时候好，有时候不好\n",
      "是一例负面评价 output=0.03\n",
      "我很孤独\n",
      "是一例负面评价 output=0.02\n",
      "陈子迅\n",
      "是一例负面评价 output=0.48\n",
      "尹俊杰\n",
      "是一例正面评价 output=0.96\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    '我觉得不好',\n",
    "    '我觉得很好',\n",
    "    '我被孤立了',\n",
    "    '我的心情有时候好，有时候不好',\n",
    "    '我很孤独',\n",
    "    '陈子迅',\n",
    "    '尹俊杰'\n",
    "]\n",
    "for text in test_list:\n",
    "    predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9279999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "X_pred = []\n",
    "for entry in model.predict(X_test):\n",
    "    if entry > 0.5:\n",
    "        X_pred.append(1)\n",
    "    else:\n",
    "        X_pred.append(0)\n",
    "print(f1_score(y_test,X_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**错误分类的文本**\n",
    "经过查看，发现错误分类的文本的含义大多比较含糊，就算人类也不容易判断极性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.T[0]\n",
    "y_pred = [1 if p>= 0.5 else 0 for p in y_pred]\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出错误分类的索引\n",
    "misclassified = np.where(y_pred != y_actual)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "99\n",
      "[ 2 11 17 23 29 31 40 69 84]\n"
     ]
    }
   ],
   "source": [
    "# 输出所有错误分类的索引\n",
    "print(len(misclassified))\n",
    "print(len(X_test))\n",
    "print(misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   弄 备份瞎 玩儿\n",
      "预测的分类 1\n",
      "实际的分类 0.0\n"
     ]
    }
   ],
   "source": [
    "# 找出错误分类的样本\n",
    "idx=40\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
