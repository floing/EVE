{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries Needed**  \n",
    "numpy\n",
    "jieba\n",
    "gensim\n",
    "tensorflow\n",
    "matplotlib\n",
    "sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba # 结巴分词\n",
    "# gensim用来加载预训练word vector\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pretrained Word Vectors**  \n",
    "使用了北京师范大学中文信息处理研究所与中国人民大学 DBIIR 实验室的研究者开源的Chinese-Word-Vectors：  \n",
    "https://github.com/Embedding/Chinese-Word-Vectors  \n",
    "word2vec的文章：  \n",
    "https://zhuanlan.zhihu.com/p/26306795  \n",
    "我们使用了\"chinese-word-vectors\"知乎Word+Ngram的词向量，可以从上面github链接下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用gensim加载预训练中文分词embedding\n",
    "cn_model = KeyedVectors.load_word2vec_format('../../sgns.zhihu.bigram',binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Vectors Model**  \n",
    "在这个词向量模型里，每一个词是一个索引，对应的是一个长度为300的向量。LSTM并不能直接处理汉字文本，需要先进行分次并把词汇转换为词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词向量的长度为300\n"
     ]
    }
   ],
   "source": [
    "# 每一个词都对应一个长度为300的向量\n",
    "embedding_dim = cn_model['孔子'].shape[0]\n",
    "print('词向量的长度为{}'.format(embedding_dim))\n",
    "# cn_model['孔子']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.521646"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算相似度\n",
    "cn_model.similarity('孔子', '庄子')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('我国', 0.6130719184875488),\n",
       " ('天朝', 0.5357025861740112),\n",
       " ('美国', 0.5048182010650635),\n",
       " ('中国人', 0.5000995993614197),\n",
       " ('本国', 0.4976556897163391),\n",
       " ('印度', 0.49548032879829407),\n",
       " ('日本', 0.491807758808136),\n",
       " ('国内', 0.4640890955924988),\n",
       " ('大陆', 0.4627026915550232),\n",
       " ('中华民族', 0.43461209535598755)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找出最相近的词，余弦相似度\n",
    "cn_model.most_similar(positive=['中国'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 孔丘 圣人 孔子 孟子 孔子 荀子 中:\n",
      "不是同一类别的词为: 圣人\n"
     ]
    }
   ],
   "source": [
    "# 找出不同的词\n",
    "test_words = '孔丘 圣人 孔子 孟子 孔子 荀子'\n",
    "test_words_result = cn_model.doesnt_match(test_words.split())\n",
    "print('在 '+test_words+' 中:\\n不是同一类别的词为: %s' %test_words_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('劈腿', 0.5849199295043945),\n",
       " ('婚外情', 0.5557921528816223),\n",
       " ('偷情', 0.5555664300918579),\n",
       " ('外遇', 0.5458645820617676),\n",
       " ('再婚', 0.5422405004501343),\n",
       " ('未婚先孕', 0.5357398986816406),\n",
       " ('隐婚', 0.5257365703582764),\n",
       " ('离婚', 0.524539053440094),\n",
       " ('马蓉', 0.5239365696907043),\n",
       " ('通奸', 0.5222055912017822)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.most_similar(positive=['女人','出轨'], negative=['男人'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 样本存放于两个文件夹中，分别为正面评价文件夹'pos'和负面评价文件夹'neg'\n",
    "# 每个文件夹中有txt文件，每个文件中是一例评价\n",
    "import os\n",
    "pos_txts = os.listdir('Mental or Casual/mental')\n",
    "neg_txts = os.listdir('Mental or Casual/casual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总共: 5492\n"
     ]
    }
   ],
   "source": [
    "print('样本总共: '+ str(len(pos_txts) + len(neg_txts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在我们将所有的评价内容放置到一个list里\n",
    "train_texts_orig = []\n",
    "for i in range(len(pos_txts)):\n",
    "    with open('Mental or Casual/mental/'+pos_txts[i], 'r', errors='ignore') as f:\n",
    "        text = f.read().strip()\n",
    "        train_texts_orig.append(text)\n",
    "        f.close()\n",
    "for i in range(len(neg_txts)):\n",
    "    with open('Mental or Casual/casual/'+neg_txts[i], 'r', errors='ignore') as f:\n",
    "        text = f.read().strip()\n",
    "        train_texts_orig.append(text)\n",
    "        f.close()\n",
    "# 添加完所有样本之后，train_texts_orig为一个含有文本的list，其中先是正面评价，后为负面评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'每天都有干不完的事情，每天九点多十点结束了碎片化和“别人的生活”之后总有种报复性熬夜的快感，喜欢熬夜写稿子，喜欢熬夜读书，喜欢熬夜用安静的整块的时间做一些自己的事。睡不睡觉反而是次要的了。'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify integrity of original data\n",
    "len(train_texts_orig)\n",
    "train_texts_orig[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用tensorflow的keras接口来建模\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分词和tokenize**  \n",
    "首先我们去掉每个样本的标点符号，然后用jieba分词，jieba分词返回一个生成器，没法直接进行tokenize，所以我们将分词结果转换成一个list，并将它索引化，这样每一例评价的文本变成一段索引数字，对应着预训练词向量模型中的词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\EdiC\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.483 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 进行分词和tokenize\n",
    "# train_tokens是一个长list，其中含有多个小list，对应每一条评价\n",
    "train_tokens = []\n",
    "for text in train_texts_orig:\n",
    "    # 去掉标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 结巴分词\n",
    "    cut = jieba.cut(text)\n",
    "    # 结巴分词的输出结果为一个生成器\n",
    "    # 把生成器转换为list\n",
    "    cut_list = [ i for i in cut ]\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**索引长度标准化**  \n",
    "因为每段评语的长度是不一样的，我们如果单纯取最长的一个评语，并把其他评填充成同样的长度，这样十分浪费计算资源，所以我们取一个折衷的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得所有tokens的长度\n",
    "num_tokens = [len(tokens) for tokens in train_tokens]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.825018208302986"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 平均tokens的长度\n",
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 57 194  10 ...   1   6   2]\n"
     ]
    }
   ],
   "source": [
    "# 最长的评价tokens的长度\n",
    "np.max(num_tokens)\n",
    "print(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(num_tokens,bins=1000)\n",
    "plt.xlim((0,100))\n",
    "plt.ylabel('number of tokens')\n",
    "plt.xlabel('length of tokens')\n",
    "plt.title('Distribution of tokens length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9608521485797523"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens的长度为236时，大约95%的样本被涵盖\n",
    "# 我们对长度不足的进行padding，超长的进行修剪\n",
    "np.sum( num_tokens < max_tokens ) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**反向tokenize**  \n",
    "我们定义一个function，用来把索引转换成可阅读的文本，这对于debug很重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来将tokens转换为文本\n",
    "def reverse_tokens(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "            text = text + cn_model.index2word[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse = reverse_tokens(train_tokens[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'同学你好看你的帖子我感受到了你内心对于美好的东西的向往以及看到一些不公平残酷事情时内心的悲伤我知道你是一个很善良的人带着善意去面对生活也希望每一个人都可以被温柔的对待当你不开心时也试着好好关爱一下自己不要把不开心憋闷在心里如果愿意的话可以来跟咨询师聊聊每周四晚八点也可以来在线咨询希望你开心'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 经过tokenize再恢复成文本\n",
    "# 可见标点符号都没有了\n",
    "reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'同学你好，看你的帖子我感受到了你内心对于美好的东西的向往，以及看到一些不公平，残酷事情时内心的悲伤。我知道你是一个很善良的人，带着善意去面对生活，也希望每一个人都可以被温柔的对待。当你不开心时，也试着好好关爱一下自己，不要把不开心憋闷在心里，如果愿意的话可以来跟咨询师聊聊，每周四晚八点也可以来在线咨询。希望你开心！'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始文本\n",
    "train_texts_orig[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**准备Embedding Matrix**  \n",
    "现在我们来为模型准备embedding matrix（词向量矩阵），根据keras的要求，我们需要准备一个维度为$(numwords, embeddingdim)$的矩阵，num words代表我们使用的词汇的数量，emdedding dimension在我们现在使用的预训练词向量模型中是300，每一个词汇都用一个长度为300的向量表示。  \n",
    "注意我们只选择使用前100k个使用频率最高的词，在这个预训练词向量模型中，一共有260万词汇量，如果全部使用在分类问题上会很浪费计算资源，因为我们的训练样本很小，一共只有4k，如果我们有100k，200k甚至更多的训练样本时，在分类问题上可以考虑减少使用的词汇量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只使用前100000个词\n",
    "num_words = 100000\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# 维度为 100000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查index是否对应，\n",
    "# 输出300意义为长度为300的embedding向量一一对应\n",
    "np.sum( cn_model[cn_model.index2word[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 300)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_matrix的维度，\n",
    "# 这个维度为keras的要求，后续会在模型中用到\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**padding（填充）和truncating（修剪）**  \n",
    "我们把文本转换为tokens（索引）之后，每一串索引的长度并不相等，所以为了方便模型的训练我们需要把索引的长度标准化，上面我们选择了236这个可以涵盖95%训练样本的长度，接下来我们进行padding和truncating，我们一般采用'pre'的方法，这会在文本索引的前面填充0，因为根据一些研究资料中的实践，如果在文本索引后面填充0的话，会对模型造成一些不良影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行padding和truncating， 输入的train_tokens是一个list\n",
    "# 返回的train_pad是一个numpy array\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超出五万个词向量的词用0代替\n",
    "train_pad[train_pad>=num_words] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,   347,    42,  2962,  2389,   834,    15,    42,   626,\n",
       "         365,     3,    87,   185,   689,     4,   176,   113,     1,\n",
       "          12,    10,   115,     5,    23,    15,   192,     3,    57,\n",
       "          81,   347,   451,     1,  2884, 26721,   185,    11, 10179,\n",
       "           3,   577,     1,  1869,   812,  9166,   314,   858,   208,\n",
       "          98,  1464,   446,   316,    23,    62,     1,   163,    46,\n",
       "         540,   366,   508,     0,   318,   101,     4,  3555,  3841,\n",
       "         139,    10,    56,   347,  3564,   139,    58,     5,    27,\n",
       "        7109,    42,   414,     1,  7330,    55,    27,   358,    42,\n",
       "         192,  3018,   153])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可见padding之后前面的tokens全变成0，文本在最后面\n",
    "train_pad[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备target向量，前1492样本为1，后4000为0\n",
    "train_target = np.concatenate((np.ones(1492),np.zeros(4000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行训练和测试样本的分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90%的样本用来训练，剩余10%用来测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad, train_target, test_size=0.1, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                              主人\n",
      "class:  0.0\n"
     ]
    }
   ],
   "source": [
    "# 查看训练样本，确认无误\n",
    "print(reverse_tokens(X_train[33]))\n",
    "print('class: ',y_train[33])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们用keras搭建LSTM模型，模型的第一层是Embedding层，只有当我们把tokens索引转换为词向量矩阵之后，才可以用神经网络对文本进行处理。\n",
    "keras提供了Embedding接口，避免了繁琐的稀疏矩阵操作。   \n",
    "在Embedding层我们输入的矩阵为：$$(batchsize, maxtokens)$$\n",
    "输出矩阵为： $$(batchsize, maxtokens, embeddingdim)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement K-fold validation, k=4\n",
    "# k = 4\n",
    "# num_val_samples = len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# 用LSTM对样本进行分类\n",
    "model = Sequential()\n",
    "\n",
    "# 模型第一层为Embedding\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Bidirectional(LSTM(units=32, return_sequences=True)))\n",
    "model.add(LSTM(units=16, return_sequences=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**构建模型**  \n",
    "我在这个教程中尝试了几种神经网络结构，因为训练样本比较少，所以我们可以尽情尝试，训练过程等待时间并不长：  \n",
    "**GRU：**如果使用GRU的话，测试样本可以达到87%的准确率，但我测试自己的文本内容时发现，GRU最后一层激活函数的输出都在0.5左右，说明模型的判断不是很明确，信心比较低，而且经过测试发现模型对于否定句的判断有时会失误，我们期望对于负面样本输出接近0，正面样本接近1而不是都徘徊于0.5之间。  \n",
    "**BiLSTM：**测试了LSTM和BiLSTM，发现BiLSTM的表现最好，LSTM的表现略好于GRU，这可能是因为BiLSTM对于比较长的句子结构有更好的记忆，有兴趣的朋友可以深入研究一下。  \n",
    "Embedding之后第，一层我们用BiLSTM返回sequences，然后第二层16个单元的LSTM不返回sequences，只返回最终结果，最后是一个全链接层，用sigmoid激活函数输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU的代码\n",
    "# model.add(GRU(units=32, return_sequences=True))\n",
    "# model.add(GRU(units=16, return_sequences=True))\n",
    "# model.add(GRU(units=4, return_sequences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# 我们使用adam以0.001的learning rate进行优化\n",
    "optimizer = Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 255, 300)          30000000  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 255, 64)           85248     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 16)                5184      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 30,090,449\n",
      "Trainable params: 90,449\n",
      "Non-trainable params: 30,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 模型的结构\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "path_checkpoint = 'sentiment_checkpoint.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to open file (unable to open file: name = 'sentiment_checkpoint.keras', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "# 尝试加载已训练模型\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义early stoping如果3个epoch内validation loss没有改善则停止训练\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动降低learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-5, patience=0,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义callback函数\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4447 samples, validate on 495 samples\n",
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.2415 - acc: 0.9331\n",
      "Epoch 00001: val_loss improved from inf to 0.08438, saving model to sentiment_checkpoint.keras\n",
      "4447/4447 [==============================] - 62s 14ms/sample - loss: 0.2385 - acc: 0.9341 - val_loss: 0.0844 - val_acc: 0.9859\n",
      "Epoch 2/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0748 - acc: 0.9837\n",
      "Epoch 00002: val_loss improved from 0.08438 to 0.05289, saving model to sentiment_checkpoint.keras\n",
      "4447/4447 [==============================] - 57s 13ms/sample - loss: 0.0752 - acc: 0.9834 - val_loss: 0.0529 - val_acc: 0.9859\n",
      "Epoch 3/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0518 - acc: 0.9878\n",
      "Epoch 00003: val_loss improved from 0.05289 to 0.03587, saving model to sentiment_checkpoint.keras\n",
      "4447/4447 [==============================] - 58s 13ms/sample - loss: 0.0529 - acc: 0.9874 - val_loss: 0.0359 - val_acc: 0.9939\n",
      "Epoch 4/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0427 - acc: 0.9894\n",
      "Epoch 00004: val_loss improved from 0.03587 to 0.03053, saving model to sentiment_checkpoint.keras\n",
      "4447/4447 [==============================] - 57s 13ms/sample - loss: 0.0421 - acc: 0.9897 - val_loss: 0.0305 - val_acc: 0.9939\n",
      "Epoch 5/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0322 - acc: 0.9924\n",
      "Epoch 00005: val_loss improved from 0.03053 to 0.02474, saving model to sentiment_checkpoint.keras\n",
      "4447/4447 [==============================] - 57s 13ms/sample - loss: 0.0321 - acc: 0.9924 - val_loss: 0.0247 - val_acc: 0.9960\n",
      "Epoch 6/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0260 - acc: 0.9949\n",
      "Epoch 00006: val_loss improved from 0.02474 to 0.02318, saving model to sentiment_checkpoint.keras\n",
      "4447/4447 [==============================] - 57s 13ms/sample - loss: 0.0255 - acc: 0.9951 - val_loss: 0.0232 - val_acc: 0.9939\n",
      "Epoch 7/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0214 - acc: 0.9966\n",
      "Epoch 00007: val_loss did not improve from 0.02318\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "4447/4447 [==============================] - 57s 13ms/sample - loss: 0.0218 - acc: 0.9964 - val_loss: 0.0241 - val_acc: 0.9960\n",
      "Epoch 8/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0188 - acc: 0.9966\n",
      "Epoch 00008: val_loss improved from 0.02318 to 0.02274, saving model to sentiment_checkpoint.keras\n",
      "4447/4447 [==============================] - 56s 13ms/sample - loss: 0.0196 - acc: 0.9964 - val_loss: 0.0227 - val_acc: 0.9960\n",
      "Epoch 9/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0191 - acc: 0.9963\n",
      "Epoch 00009: val_loss improved from 0.02274 to 0.02270, saving model to sentiment_checkpoint.keras\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "4447/4447 [==============================] - 57s 13ms/sample - loss: 0.0187 - acc: 0.9964 - val_loss: 0.0227 - val_acc: 0.9960\n",
      "Epoch 10/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0187 - acc: 0.9963\n",
      "Epoch 00010: val_loss improved from 0.02270 to 0.02262, saving model to sentiment_checkpoint.keras\n",
      "4447/4447 [==============================] - 56s 13ms/sample - loss: 0.0184 - acc: 0.9964 - val_loss: 0.0226 - val_acc: 0.9960\n",
      "Epoch 11/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0186 - acc: 0.9963\n",
      "Epoch 00011: val_loss improved from 0.02262 to 0.02257, saving model to sentiment_checkpoint.keras\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "4447/4447 [==============================] - 55s 12ms/sample - loss: 0.0183 - acc: 0.9964 - val_loss: 0.0226 - val_acc: 0.9960\n",
      "Epoch 12/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0185 - acc: 0.9963\n",
      "Epoch 00012: val_loss improved from 0.02257 to 0.02252, saving model to sentiment_checkpoint.keras\n",
      "4447/4447 [==============================] - 54s 12ms/sample - loss: 0.0182 - acc: 0.9964 - val_loss: 0.0225 - val_acc: 0.9960\n",
      "Epoch 13/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0185 - acc: 0.9963\n",
      "Epoch 00013: val_loss improved from 0.02252 to 0.02248, saving model to sentiment_checkpoint.keras\n",
      "4447/4447 [==============================] - 56s 13ms/sample - loss: 0.0182 - acc: 0.9964 - val_loss: 0.0225 - val_acc: 0.9960\n",
      "Epoch 14/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0179 - acc: 0.9966\n",
      "Epoch 00014: val_loss improved from 0.02248 to 0.02247, saving model to sentiment_checkpoint.keras\n",
      "4447/4447 [==============================] - 59s 13ms/sample - loss: 0.0181 - acc: 0.9964 - val_loss: 0.0225 - val_acc: 0.9960\n",
      "Epoch 15/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0163 - acc: 0.9968\n",
      "Epoch 00015: val_loss improved from 0.02247 to 0.02243, saving model to sentiment_checkpoint.keras\n",
      "4447/4447 [==============================] - 55s 12ms/sample - loss: 0.0181 - acc: 0.9964 - val_loss: 0.0224 - val_acc: 0.9960\n",
      "Epoch 16/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0173 - acc: 0.9966\n",
      "Epoch 00016: val_loss improved from 0.02243 to 0.02243, saving model to sentiment_checkpoint.keras\n",
      "4447/4447 [==============================] - 53s 12ms/sample - loss: 0.0180 - acc: 0.9964 - val_loss: 0.0224 - val_acc: 0.9960\n",
      "Epoch 17/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0183 - acc: 0.9963\n",
      "Epoch 00017: val_loss improved from 0.02243 to 0.02240, saving model to sentiment_checkpoint.keras\n",
      "4447/4447 [==============================] - 52s 12ms/sample - loss: 0.0180 - acc: 0.9964 - val_loss: 0.0224 - val_acc: 0.9960\n",
      "Epoch 18/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0182 - acc: 0.9966\n",
      "Epoch 00018: val_loss improved from 0.02240 to 0.02239, saving model to sentiment_checkpoint.keras\n",
      "4447/4447 [==============================] - 53s 12ms/sample - loss: 0.0179 - acc: 0.9966 - val_loss: 0.0224 - val_acc: 0.9960\n",
      "Epoch 19/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0181 - acc: 0.9966\n",
      "Epoch 00019: val_loss improved from 0.02239 to 0.02235, saving model to sentiment_checkpoint.keras\n",
      "4447/4447 [==============================] - 54s 12ms/sample - loss: 0.0179 - acc: 0.9966 - val_loss: 0.0224 - val_acc: 0.9960\n",
      "Epoch 20/20\n",
      "4352/4447 [============================>.] - ETA: 1s - loss: 0.0181 - acc: 0.9966\n",
      "Epoch 00020: val_loss improved from 0.02235 to 0.02234, saving model to sentiment_checkpoint.keras\n",
      "4447/4447 [==============================] - 53s 12ms/sample - loss: 0.0178 - acc: 0.9966 - val_loss: 0.0223 - val_acc: 0.9960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x233b079af98>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          validation_split=0.1, \n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**结论**  \n",
    "我们首先对测试样本进行预测，得到了还算满意的准确度。  \n",
    "之后我们定义一个预测函数，来预测输入的文本的极性，可见模型对于否定句和一些简单的逻辑结构都可以进行准确的判断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550/550 [==============================] - 3s 6ms/sample - loss: 0.0172 - acc: 0.9964\n",
      "Accuracy:0.99636364\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:'+str(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('是心理相关的','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('是相对闲聊的','output=%.2f'%coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "再接再厉，相同行为背后的原因是不一样的，再接再厉，相同行为背后的原因是不一样的再接再厉，相同行为背后的原因是不一样的再接再厉，相同行为背后的原因是不一样的\n",
      "是心理相关的 output=0.99\n",
      "你好\n",
      "是相对闲聊的 output=0.00\n",
      "你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好\n",
      "是心理相关的 output=0.92\n",
      "我的心情有时候好，有时候不好\n",
      "是相对闲聊的 output=0.34\n",
      "我很孤独\n",
      "是相对闲聊的 output=0.01\n",
      "陈子迅\n",
      "是相对闲聊的 output=0.01\n",
      "再接再厉，相同行为背后的原因是不一样的\n",
      "是相对闲聊的 output=0.02\n",
      "同学你好\n",
      "是相对闲聊的 output=0.00\n",
      "同学你好同学你好同学你好同学你好同学你好同学你好同学你好同学你好同学你好同学你好同学你好同学你好同学你好同学你好同学你好同学你好\n",
      "是心理相关的 output=0.99\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    '再接再厉，相同行为背后的原因是不一样的，再接再厉，相同行为背后的原因是不一样的再接再厉，相同行为背后的原因是不一样的再接再厉，相同行为背后的原因是不一样的',\n",
    "    '你好',\n",
    "    '你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好你好',\n",
    "    '我的心情有时候好，有时候不好',\n",
    "    '我很孤独',\n",
    "    '陈子迅',\n",
    "    '再接再厉，相同行为背后的原因是不一样的',\n",
    "    '同学你好',\n",
    "    '同学你好同学你好同学你好同学你好同学你好同学你好同学你好同学你好同学你好同学你好同学你好同学你好同学你好同学你好同学你好同学你好'\n",
    "]\n",
    "for text in test_list:\n",
    "    predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **错误分类的文本**\n",
    "经过查看，发现错误分类的文本的含义大多比较含糊，就算人类也不容易判断极性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.T[0]\n",
    "y_pred = [1 if p>= 0.5 else 0 for p in y_pred]\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出错误分类的索引\n",
    "misclassified = np.where(y_pred != y_actual)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "# 输出所有错误分类的索引\n",
    "len(misclassified)\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           选诚恳的人\n",
      "预测的分类 1\n",
      "实际的分类 1.0\n"
     ]
    }
   ],
   "source": [
    "# 找出错误分类的样本\n",
    "idx=3\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            氪金打游戏\n",
      "预测的分类 1\n",
      "实际的分类 0.0\n"
     ]
    }
   ],
   "source": [
    "idx=1\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
