{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries Needed**  \n",
    "numpy\n",
    "jieba\n",
    "gensim\n",
    "tensorflow\n",
    "matplotlib\n",
    "sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba # 结巴分词\n",
    "# gensim用来加载预训练word vector\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pretrained Word Vectors**  \n",
    "使用了北京师范大学中文信息处理研究所与中国人民大学 DBIIR 实验室的研究者开源的Chinese-Word-Vectors：  \n",
    "https://github.com/Embedding/Chinese-Word-Vectors  \n",
    "word2vec的文章：  \n",
    "https://zhuanlan.zhihu.com/p/26306795  \n",
    "我们使用了\"chinese-word-vectors\"知乎Word+Ngram的词向量，可以从上面github链接下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用gensim加载预训练中文分词embedding\n",
    "cn_model = KeyedVectors.load_word2vec_format('../../sgns.zhihu.bigram',binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Vectors Model**  \n",
    "在这个词向量模型里，每一个词是一个索引，对应的是一个长度为300的向量。LSTM并不能直接处理汉字文本，需要先进行分次并把词汇转换为词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词向量的长度为300\n"
     ]
    }
   ],
   "source": [
    "# 每一个词都对应一个长度为300的向量\n",
    "embedding_dim = cn_model['孔子'].shape[0]\n",
    "print('词向量的长度为{}'.format(embedding_dim))\n",
    "# cn_model['孔子']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.521646"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算相似度\n",
    "cn_model.similarity('孔子', '庄子')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('我国', 0.6130719184875488),\n",
       " ('天朝', 0.5357025861740112),\n",
       " ('美国', 0.5048182010650635),\n",
       " ('中国人', 0.5000995993614197),\n",
       " ('本国', 0.4976556897163391),\n",
       " ('印度', 0.49548032879829407),\n",
       " ('日本', 0.491807758808136),\n",
       " ('国内', 0.4640890955924988),\n",
       " ('大陆', 0.4627026915550232),\n",
       " ('中华民族', 0.43461209535598755)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找出最相近的词，余弦相似度\n",
    "cn_model.most_similar(positive=['中国'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 孔丘 圣人 孔子 孟子 孔子 荀子 中:\n",
      "不是同一类别的词为: 圣人\n"
     ]
    }
   ],
   "source": [
    "# 找出不同的词\n",
    "test_words = '孔丘 圣人 孔子 孟子 孔子 荀子'\n",
    "test_words_result = cn_model.doesnt_match(test_words.split())\n",
    "print('在 '+test_words+' 中:\\n不是同一类别的词为: %s' %test_words_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('劈腿', 0.5849199295043945)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.most_similar(positive=['女人','出轨'], negative=['男人'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 样本存放于两个文件夹中，分别为正面评价文件夹'pos'和负面评价文件夹'neg'\n",
    "# 每个文件夹中有txt文件，每个文件中是一例评价\n",
    "import os\n",
    "pos_txts = os.listdir('Mental or Casual/mental')\n",
    "neg_txts = os.listdir('Mental or Casual/casual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总共: 106757\n"
     ]
    }
   ],
   "source": [
    "print('样本总共: '+ str(len(pos_txts) + len(neg_txts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在我们将所有的评价内容放置到一个list里\n",
    "train_texts_orig = []\n",
    "for i in range(len(pos_txts)):\n",
    "    with open('Mental or Casual/mental/'+pos_txts[i], 'r', errors='ignore') as f:\n",
    "        text = f.read().strip()\n",
    "        train_texts_orig.append(text)\n",
    "        f.close()\n",
    "for i in range(len(neg_txts)):\n",
    "    with open('Mental or Casual/casual/'+neg_txts[i], 'r', errors='ignore') as f:\n",
    "        text = f.read().strip()\n",
    "        train_texts_orig.append(text)\n",
    "        f.close()\n",
    "# 添加完所有样本之后，train_texts_orig为一个含有文本的list，其中先是正面评价，后为负面评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'每天都有干不完的事情'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify integrity of original data\n",
    "len(train_texts_orig)\n",
    "train_texts_orig[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用tensorflow的keras接口来建模\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分词和tokenize**  \n",
    "首先我们去掉每个样本的标点符号，然后用jieba分词，jieba分词返回一个生成器，没法直接进行tokenize，所以我们将分词结果转换成一个list，并将它索引化，这样每一例评价的文本变成一段索引数字，对应着预训练词向量模型中的词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\EdiC\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.208 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 进行分词和tokenize\n",
    "# train_tokens是一个长list，其中含有多个小list，对应每一条评价\n",
    "train_tokens = []\n",
    "for text in train_texts_orig:\n",
    "    # 去掉标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 结巴分词\n",
    "    cut = jieba.cut(text)\n",
    "    # 结巴分词的输出结果为一个生成器\n",
    "    # 把生成器转换为list\n",
    "    cut_list = [ i for i in cut ]\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**索引长度标准化**  \n",
    "因为每段评语的长度是不一样的，我们如果单纯取最长的一个评语，并把其他评填充成同样的长度，这样十分浪费计算资源，所以我们取一个折衷的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得所有tokens的长度\n",
    "num_tokens = [len(tokens) for tokens in train_tokens]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.921410305647405"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 平均tokens的长度\n",
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8  2  9 ...  9  3 11]\n"
     ]
    }
   ],
   "source": [
    "# 最长的评价tokens的长度\n",
    "np.max(num_tokens)\n",
    "print(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(num_tokens,bins=1000)\n",
    "plt.xlim((0,20))\n",
    "plt.ylabel('number of tokens')\n",
    "plt.xlabel('length of tokens')\n",
    "plt.title('Distribution of tokens length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9581854117294416"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens的长度为236时，大约95%的样本被涵盖\n",
    "# 我们对长度不足的进行padding，超长的进行修剪\n",
    "np.sum( num_tokens < max_tokens ) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**反向tokenize**  \n",
    "我们定义一个function，用来把索引转换成可阅读的文本，这对于debug很重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来将tokens转换为文本\n",
    "def reverse_tokens(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "            text = text + cn_model.index2word[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse = reverse_tokens(train_tokens[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'就会产生浓浓的偷窥欲'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 经过tokenize再恢复成文本\n",
    "# 可见标点符号都没有了\n",
    "reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'就会产生浓浓的偷窥欲'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始文本\n",
    "train_texts_orig[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**准备Embedding Matrix**  \n",
    "现在我们来为模型准备embedding matrix（词向量矩阵），根据keras的要求，我们需要准备一个维度为$(numwords, embeddingdim)$的矩阵，num words代表我们使用的词汇的数量，emdedding dimension在我们现在使用的预训练词向量模型中是300，每一个词汇都用一个长度为300的向量表示。  \n",
    "注意我们只选择使用前100k个使用频率最高的词，在这个预训练词向量模型中，一共有260万词汇量，如果全部使用在分类问题上会很浪费计算资源，因为我们的训练样本很小，一共只有4k，如果我们有100k，200k甚至更多的训练样本时，在分类问题上可以考虑减少使用的词汇量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只使用前100000个词\n",
    "num_words = 100000\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# 维度为 100000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查index是否对应，\n",
    "# 输出300意义为长度为300的embedding向量一一对应\n",
    "np.sum( cn_model[cn_model.index2word[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 300)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_matrix的维度，\n",
    "# 这个维度为keras的要求，后续会在模型中用到\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**padding（填充）和truncating（修剪）**  \n",
    "我们把文本转换为tokens（索引）之后，每一串索引的长度并不相等，所以为了方便模型的训练我们需要把索引的长度标准化，上面我们选择了236这个可以涵盖95%训练样本的长度，接下来我们进行padding和truncating，我们一般采用'pre'的方法，这会在文本索引的前面填充0，因为根据一些研究资料中的实践，如果在文本索引后面填充0的话，会对模型造成一些不良影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行padding和truncating， 输入的train_tokens是一个list\n",
    "# 返回的train_pad是一个numpy array\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超出五万个词向量的词用0代替\n",
    "train_pad[train_pad>=num_words] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,   316,  1818, 76451,   433, 27237, 13478])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可见padding之后前面的tokens全变成0，文本在最后面\n",
    "train_pad[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备target向量，前32477样本为1，后74280为0\n",
    "train_target = np.concatenate((np.ones(32477),np.zeros(74280)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行训练和测试样本的分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90%的样本用来训练，剩余10%用来测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad, train_target, test_size=0.1, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         而我对于此事的悲观看法\n",
      "class:  1.0\n"
     ]
    }
   ],
   "source": [
    "# 查看训练样本，确认无误\n",
    "print(reverse_tokens(X_train[8]))\n",
    "print('class: ',y_train[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们用keras搭建LSTM模型，模型的第一层是Embedding层，只有当我们把tokens索引转换为词向量矩阵之后，才可以用神经网络对文本进行处理。\n",
    "keras提供了Embedding接口，避免了繁琐的稀疏矩阵操作。   \n",
    "在Embedding层我们输入的矩阵为：$$(batchsize, maxtokens)$$\n",
    "输出矩阵为： $$(batchsize, maxtokens, embeddingdim)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# 用LSTM对样本进行分类\n",
    "model = Sequential()\n",
    "\n",
    "# 模型第一层为Embedding\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Bidirectional(LSTM(units=32, return_sequences=True)))\n",
    "model.add(LSTM(units=16, return_sequences=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**构建模型**  \n",
    "我在这个教程中尝试了几种神经网络结构，因为训练样本比较少，所以我们可以尽情尝试，训练过程等待时间并不长：  \n",
    "**GRU：**如果使用GRU的话，测试样本可以达到87%的准确率，但我测试自己的文本内容时发现，GRU最后一层激活函数的输出都在0.5左右，说明模型的判断不是很明确，信心比较低，而且经过测试发现模型对于否定句的判断有时会失误，我们期望对于负面样本输出接近0，正面样本接近1而不是都徘徊于0.5之间。  \n",
    "**BiLSTM：**测试了LSTM和BiLSTM，发现BiLSTM的表现最好，LSTM的表现略好于GRU，这可能是因为BiLSTM对于比较长的句子结构有更好的记忆，有兴趣的朋友可以深入研究一下。  \n",
    "Embedding之后第，一层我们用BiLSTM返回sequences，然后第二层16个单元的LSTM不返回sequences，只返回最终结果，最后是一个全链接层，用sigmoid激活函数输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU的代码\n",
    "# model.add(GRU(units=32, return_sequences=True))\n",
    "# model.add(GRU(units=16, return_sequences=True))\n",
    "# model.add(GRU(units=4, return_sequences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# 我们使用adam以0.001的learning rate进行优化\n",
    "optimizer = Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 16, 300)           30000000  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 16, 64)            85248     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 16)                5184      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 30,090,449\n",
      "Trainable params: 90,449\n",
      "Non-trainable params: 30,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 模型的结构\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "path_checkpoint = 'sentiment_checkpoint.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to open file (unable to open file: name = 'sentiment_checkpoint.keras', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "# 尝试加载已训练模型\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义early stoping如果3个epoch内validation loss没有改善则停止训练\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动降低learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-5, patience=0,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义callback函数\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86472 samples, validate on 9609 samples\n",
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "86400/86472 [============================>.] - ETA: 0s - loss: 0.3055 - acc: 0.8617\n",
      "Epoch 00001: val_loss improved from inf to 0.26318, saving model to sentiment_checkpoint.keras\n",
      "86472/86472 [==============================] - 61s 707us/sample - loss: 0.3054 - acc: 0.8618 - val_loss: 0.2632 - val_acc: 0.8805\n",
      "Epoch 2/20\n",
      "86400/86472 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.8882\n",
      "Epoch 00002: val_loss improved from 0.26318 to 0.25197, saving model to sentiment_checkpoint.keras\n",
      "86472/86472 [==============================] - 61s 701us/sample - loss: 0.2469 - acc: 0.8882 - val_loss: 0.2520 - val_acc: 0.8860\n",
      "Epoch 3/20\n",
      "86400/86472 [============================>.] - ETA: 0s - loss: 0.2279 - acc: 0.8975\n",
      "Epoch 00003: val_loss improved from 0.25197 to 0.25036, saving model to sentiment_checkpoint.keras\n",
      "86472/86472 [==============================] - 55s 635us/sample - loss: 0.2279 - acc: 0.8975 - val_loss: 0.2504 - val_acc: 0.8881\n",
      "Epoch 4/20\n",
      "86400/86472 [============================>.] - ETA: 0s - loss: 0.2121 - acc: 0.9053\n",
      "Epoch 00004: val_loss improved from 0.25036 to 0.24388, saving model to sentiment_checkpoint.keras\n",
      "86472/86472 [==============================] - 58s 673us/sample - loss: 0.2120 - acc: 0.9053 - val_loss: 0.2439 - val_acc: 0.8904\n",
      "Epoch 5/20\n",
      "86400/86472 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9142\n",
      "Epoch 00005: val_loss did not improve from 0.24388\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "86472/86472 [==============================] - 66s 761us/sample - loss: 0.1952 - acc: 0.9142 - val_loss: 0.2488 - val_acc: 0.8916\n",
      "Epoch 6/20\n",
      "86400/86472 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9295\n",
      "Epoch 00006: val_loss did not improve from 0.24388\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "86472/86472 [==============================] - 66s 767us/sample - loss: 0.1659 - acc: 0.9295 - val_loss: 0.2522 - val_acc: 0.8943\n",
      "Epoch 7/20\n",
      "86400/86472 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9337\n",
      "Epoch 00007: val_loss did not improve from 0.24388\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "86472/86472 [==============================] - 71s 823us/sample - loss: 0.1584 - acc: 0.9337 - val_loss: 0.2521 - val_acc: 0.8946\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1591ce94f28>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          validation_split=0.1, \n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**结论**  \n",
    "我们首先对测试样本进行预测，得到了还算满意的准确度。  \n",
    "之后我们定义一个预测函数，来预测输入的文本的极性，可见模型对于否定句和一些简单的逻辑结构都可以进行准确的判断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10676/10676 [==============================] - 5s 433us/sample - loss: 0.2406 - acc: 0.9015\n",
      "Accuracy:0.90146124\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:'+str(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('是心理相关的','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('是相对闲聊的','output=%.2f'%coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "再接再厉，相同行为背后的原因是不一样的，再接再厉，相同行为背后的原因是不一样的再接再厉，相同行为背后的原因是不一样的再接再厉，相同行为背后的原因是不一样的\n",
      "是心理相关的 output=0.96\n",
      "你好\n",
      "是心理相关的 output=0.99\n",
      "我们访问了海南\n",
      "是相对闲聊的 output=0.04\n",
      "我的心情有时候好，有时候不好\n",
      "是心理相关的 output=0.93\n",
      "我很孤独\n",
      "是心理相关的 output=0.90\n",
      "作者：几个高中生\n",
      "是相对闲聊的 output=0.01\n",
      "相同行为背后的原因是不一样的\n",
      "是心理相关的 output=0.98\n",
      "这真是个笑话\n",
      "是相对闲聊的 output=0.15\n",
      "我的人生就是个笑话\n",
      "是相对闲聊的 output=0.43\n",
      "同学你好\n",
      "是心理相关的 output=1.00\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    '再接再厉，相同行为背后的原因是不一样的，再接再厉，相同行为背后的原因是不一样的再接再厉，相同行为背后的原因是不一样的再接再厉，相同行为背后的原因是不一样的',\n",
    "    '你好',\n",
    "    '我们访问了海南',\n",
    "    '我的心情有时候好，有时候不好',\n",
    "    '我很孤独',\n",
    "    '作者：几个高中生',\n",
    "    '相同行为背后的原因是不一样的',\n",
    "    '这真是个笑话',\n",
    "    '我的人生就是个笑话',\n",
    "    '同学你好'\n",
    "]\n",
    "for text in test_list:\n",
    "    predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **错误分类的文本**\n",
    "经过查看，发现错误分类的文本的含义大多比较含糊，就算人类也不容易判断极性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.T[0]\n",
    "y_pred = [1 if p>= 0.5 else 0 for p in y_pred]\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出错误分类的索引\n",
    "misclassified = np.where(y_pred != y_actual)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1052\n",
      "10676\n"
     ]
    }
   ],
   "source": [
    "# 输出所有错误分类的索引\n",
    "print(len(misclassified))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        我不希望它发生在任何人身上\n",
      "预测的分类 1\n",
      "实际的分类 0.0\n"
     ]
    }
   ],
   "source": [
    "# 找出错误分类的样本\n",
    "idx=1034\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           简言之就是很多 上\n",
      "预测的分类 0\n",
      "实际的分类 1.0\n"
     ]
    }
   ],
   "source": [
    "idx=29\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
